{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el5r8FpE6MLm"
      },
      "source": [
        "# Lab 7: Introduction to Implicit Neural Representations (INRs)\n",
        "Welcome to week 7 of COMSM0159 Advanced Visual AI!\n",
        "\n",
        "The goal of this labsheet is to provide an introduction to Implicit Neural Representations (INRs). By the end of this lab, you should be able to do the following:\n",
        "1. Understand how INRs work and build a simple INR for image representation.\n",
        "2. Experiment with your INR model for image deblurring.\n",
        "3. Analyze the memory efficiency of INRs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vQRmVq16MLm"
      },
      "source": [
        "## Section 1: INR for Image Representation\n",
        "Implicit Neural Representations (INRs) formulates a neural network (e.g., MLP) that can take the spatial co-ordinates of a signal and output the corresponding signal value. For 2D images, an INR learns an implicit function that takes pixel coordinates as input and returns the corresponding (R, G, B) values for that location. The memory required for such a representation scales with the complexity of the signal.\n",
        "\n",
        "In this lab we will experiment with SIREN (Implicit Neural Activations with Periodic Activation Functions) as our main method. SIREN replaces the ReLU function in the MLP with a sine function as the activation function in the neural network layers, allowing it to model signals with fine details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DovTDDOv6MLm"
      },
      "outputs": [],
      "source": [
        "# some imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from scipy.ndimage import laplace, sobel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image, ImageFilter\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_rWndi46MLn"
      },
      "source": [
        "### Step 1 Initialize SIREN Layers\n",
        "The sine layer is the basic building block of SIREN. SIREN is essentially a MLP (multi-layer perceptron) with sine activation. Note that SIREN is highly dependent on weights initialization to preserve the distribution of activations through the network. The authors propose the following initializations (check section 3.2 in the paper if you are interested):\n",
        "\n",
        "* Weight is uniformly distributed such that\n",
        "$$\n",
        "w_i \\sim U\\left(-\\frac{c}{\\sqrt{n}}, \\frac{c}{\\sqrt{n}}\\right)\n",
        "$$\n",
        "where c = 6.\n",
        "    \n",
        "* Initialize the first layer of the sine network with weights so that the sine function can be:\n",
        "$$\n",
        "\\sin(\\omega_0 \\cdot W\\mathbf{x} + b)\n",
        "$$\n",
        "where `omega_0` = 30.\n",
        "  \n",
        "To simplify, the key point here is that these initializations ensure the input to each sine activation is normally distributed with a standard deviation of 1, while the output of a SIREN is always arcsine distributed within the range of [-1, 1].\n",
        "\n",
        "**Task 1:** Now complete the forward function below. Follow the equation provided below to complete the forward function. Try using [torch.sin()](https://pytorch.org/docs/stable/generated/torch.sin.html) as your sine activation function.\n",
        "$$\n",
        "\\sin(\\omega_0 \\cdot W\\mathbf{x} + b)\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmGcEweH6MLn"
      },
      "outputs": [],
      "source": [
        "class SineLayer(nn.Module):\n",
        "\n",
        "    \"\"\" Linear layer followed by the sine activation\n",
        "\n",
        "    If `is_first == True`, then it represents the first layer of the network.\n",
        "    In this case, omega_0 is a frequency factor, which simply multiplies the activations before the nonlinearity.\n",
        "    Note that it influences the initialization scheme.\n",
        "\n",
        "    If `is_first == False`, then the weights will be divided by omega_0 so as to keep the magnitude of activations constant,\n",
        "    but boost gradients to the weight matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "\n",
        "        self.in_features = in_features\n",
        "        # Initialize a linear layer with specified input and output features\n",
        "        # 'bias' indicates whether to include a bias term\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        self.init_weights()\n",
        "\n",
        "    # initialize weights uniformly\n",
        "    def init_weights(self):\n",
        "        # diasble gradient calculation in initialization\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features,\n",
        "                                             1 / self.in_features)\n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Task 1 TODO\n",
        "        # 1. pass input through linear layer (self.linear layer performs the linear transformation on the input)\n",
        "\n",
        "        # 2. scale the output of the linear transformation by the frequency factor\n",
        "\n",
        "        # 3. apply sine activation\n",
        "\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R_nhJlB6MLn"
      },
      "source": [
        "Now let's build up SIREN layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0T49YpG6MLn"
      },
      "outputs": [],
      "source": [
        "class Siren(nn.Module):\n",
        "    \"\"\" SIREN architecture \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, hidden_features=256, hidden_layers=3, outermost_linear=False,\n",
        "                 first_omega_0=30, hidden_omega_0=30.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = []\n",
        "        # add the first layer\n",
        "        self.net.append(SineLayer(in_features, hidden_features,\n",
        "                                  is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "        # append hidden layers\n",
        "        for i in range(hidden_layers):\n",
        "            self.net.append(SineLayer(hidden_features, hidden_features,\n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "\n",
        "        if outermost_linear:\n",
        "            # add a final Linear layer\n",
        "            final_linear = nn.Linear(hidden_features, out_features)\n",
        "\n",
        "            with torch.no_grad(): # weights intialization\n",
        "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0,\n",
        "                                              np.sqrt(6 / hidden_features) / hidden_omega_0)\n",
        "\n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            # otherwise, add a SineLayer\n",
        "            self.net.append(SineLayer(hidden_features, out_features,\n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        self.net = nn.Sequential(*self.net) # sequential wrapper of SineLayer and Linear\n",
        "\n",
        "    def forward(self, coords):\n",
        "        # coords represents the 2D pixel coordinates\n",
        "        coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
        "        output = self.net(coords)\n",
        "        return output, coords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhzxFXI56MLn"
      },
      "source": [
        "### Step 2 Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMrzMg0e6MLn"
      },
      "source": [
        "Let's generate a grid of coordinates over a 2D space and reshape the output into a flattened format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKbusZbr6MLn"
      },
      "outputs": [],
      "source": [
        "def get_mgrid(sidelen1,sidelen2, dim=2):\n",
        "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n",
        "    sidelen: int\n",
        "    dim: int'''\n",
        "\n",
        "    if sidelen1 >= sidelen2:\n",
        "      # use sidelen1 steps to generate the grid\n",
        "      tensors = tuple(dim * [torch.linspace(-1, 1, steps = sidelen1)])\n",
        "      mgrid = torch.stack(torch.meshgrid(*tensors), dim = -1)\n",
        "      # crop it along one axis to fit sidelen2\n",
        "      minor = int((sidelen1 - sidelen2)/2)\n",
        "      mgrid = mgrid[:,minor:sidelen2 + minor]\n",
        "\n",
        "    if sidelen1 < sidelen2:\n",
        "      tensors = tuple(dim * [torch.linspace(-1, 1, steps = sidelen2)])\n",
        "      mgrid = torch.stack(torch.meshgrid(*tensors), dim = -1)\n",
        "\n",
        "      minor = int((sidelen2 - sidelen1)/2)\n",
        "      mgrid = mgrid[minor:sidelen1 + minor,:]\n",
        "\n",
        "    # flatten the gird\n",
        "    mgrid = mgrid.reshape(-1, dim)\n",
        "\n",
        "    return mgrid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJvMNugA6MLn"
      },
      "source": [
        "Make sure we convert the input to tensor and do normalization using transformations to the range [-1, 1], which corresponds to a mean of 0.5 and a standard deviation of 0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl80YTJh6MLn"
      },
      "outputs": [],
      "source": [
        "def image_to_tensor(img):\n",
        "    transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize(torch.Tensor([0.5]), torch.Tensor([0.5]))\n",
        "    ])\n",
        "    img = transform(img)\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPjtN4sv6MLo"
      },
      "outputs": [],
      "source": [
        "# Image Fitting Dataloader\n",
        "class ImageData(Dataset):\n",
        "    def __init__(self, img):\n",
        "        super().__init__()\n",
        "\n",
        "        # convert the image to a tensor with transformations\n",
        "        img = image_to_tensor(img)\n",
        "\n",
        "        self.pixels = img.permute(1, 2, 0).reshape(-1, 3) # pixel values of the org img\n",
        "\n",
        "        # create a grid of coordinates for the image\n",
        "        self.coords = get_mgrid(img.shape[1], img.shape[2], 2)\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx > 0: raise IndexError\n",
        "\n",
        "        return self.coords, self.pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Pp1IgY6MLo"
      },
      "source": [
        "We are going to load and train an image from the [CelebFaces Attributes Dataset (CelebA)](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset. CelebA is a large-scale face attributes dataset with more than 200K celebrity images.\n",
        "\n",
        "To save you some time, you can directly download images from the celebDataset from the cloud below (a total of three celebrity face images are prepared for this lab: face1.jpg, face2.jpg, and face3.jpg for selection), or feel free to use any image you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i0C6wS-6MLo",
        "outputId": "d594e22b-f664-4017-e137-85c1631928df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-30 18:04:30--  https://drive.google.com/uc?export=download&id=1g6q8rfTUVR6o3nGyIXVpAADwVBke6ZAB\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.69.100, 173.194.69.138, 173.194.69.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.69.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1g6q8rfTUVR6o3nGyIXVpAADwVBke6ZAB&export=download [following]\n",
            "--2024-10-30 18:04:30--  https://drive.usercontent.google.com/download?id=1g6q8rfTUVR6o3nGyIXVpAADwVBke6ZAB&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 108.177.119.132, 2a00:1450:4013:c00::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|108.177.119.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11440 (11K) [image/jpeg]\n",
            "Saving to: ‘face1.jpg’\n",
            "\n",
            "face1.jpg           100%[===================>]  11.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-30 18:04:32 (83.0 MB/s) - ‘face1.jpg’ saved [11440/11440]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Option 1: Load an example CelebA image from cloud\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1g6q8rfTUVR6o3nGyIXVpAADwVBke6ZAB' -O face1.jpg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let the first image be our input\n",
        "image = Image.open('face1.jpg')\n",
        "\n",
        "# Get width and height\n",
        "width, height = image.size\n",
        "print(f\"Width: {width}, Height: {height}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiWMtGh9FBOn",
        "outputId": "29307a21-15b5-487c-9197-c2846d727358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Width: 178, Height: 218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 2: Upload your own image\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "W7NaggnciPFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 2 cont.\n",
        "# image = Image.open('yourimage.jpg') # use any image you like\n",
        "\n",
        "# # Get width and height\n",
        "# width, height = image.size\n",
        "# print(f\"Width: {width}, Height: {height}\")\n"
      ],
      "metadata": {
        "id": "leLZQT47isxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBZmidklVS5l"
      },
      "source": [
        "**Task 2:** Visualize the input image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4q9Ho0-VRYK"
      },
      "outputs": [],
      "source": [
        "# Task 2 TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 Training and Fitting an Image"
      ],
      "metadata": {
        "id": "0BYVrOUNFiZw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9VYpkX9GY2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b6d515-445b-452a-eca4-9cc37ffc10b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "# set the device to 'cuda' if available, otherwise 'cpu'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# create an ImageData instance\n",
        "celeb = ImageData(image)\n",
        "dataloader = DataLoader(celeb, batch_size=1, pin_memory=True, num_workers=0)\n",
        "\n",
        "# initialize a SIREN model\n",
        "img_siren = Siren(in_features=2, out_features=3, hidden_features=256,\n",
        "                  hidden_layers=3, outermost_linear=True)\n",
        "img_siren = img_siren.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ixgSyXO6MLo"
      },
      "outputs": [],
      "source": [
        "# define differential operators that allow us to leverage autograd to compute gradients, the laplacian, etc.\n",
        "def laplace(y, x):\n",
        "    grad = gradient(y, x)\n",
        "    return divergence(grad, x)\n",
        "\n",
        "\n",
        "def divergence(y, x):\n",
        "    div = 0.\n",
        "    for i in range(y.shape[-1]):\n",
        "        div += torch.autograd.grad(y[..., i], x, torch.ones_like(y[..., i]), create_graph=True)[0][..., i:i+1]\n",
        "    return div\n",
        "\n",
        "\n",
        "def gradient(y, x, grad_outputs=None):\n",
        "    if grad_outputs is None:\n",
        "        grad_outputs = torch.ones_like(y)\n",
        "    grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmYF36sI6MLo"
      },
      "source": [
        "**Task 3:**\n",
        "\n",
        "* Compute the gradient and the Laplacian of the output.\n",
        "\n",
        "* Visualize the model output, gradient output and laplacian output.\n",
        "\n",
        "*  What patterns do you notice in the gradient output compared to the model output? How do the edges detected by the Laplacian correlate with features in the model output?\n",
        "\n",
        "Answer: able to explain where the model is sensitive to changes in input, and describe Laplacian (second derivative) can highlight areas of rapid intensity change - edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJbWHqxg6MLo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "fa90a764-f533-40c7-cb05-853d58d257ba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "linear(): argument 'input' (position 1) must be Tensor, not NoneType",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-af1232a5d021>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_siren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-d0b883cc5e52>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, coords)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# coords represents the 2D pixel coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# allows to take derivative w.r.t. input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not NoneType"
          ]
        }
      ],
      "source": [
        "total_steps = 500 # Since the whole image is our dataset, this just means 500 gradient descent steps.\n",
        "steps_til_summary = 10\n",
        "\n",
        "optim = torch.optim.Adam(lr=1e-4, params=img_siren.parameters())\n",
        "\n",
        "model_input, ground_truth = next(iter(dataloader))\n",
        "model_input, ground_truth = model_input.to(device), ground_truth.to(device)\n",
        "\n",
        "for step in range(total_steps):\n",
        "    model_output, coords = img_siren(model_input)\n",
        "    loss = ((model_output - ground_truth)**2).mean()\n",
        "\n",
        "    if not step % steps_til_summary:\n",
        "\n",
        "        print(\"Step %d, Total loss %0.6f\" % (step, loss))\n",
        "        # Task 3 TODO\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppdKs5SA6MLo"
      },
      "source": [
        "## Section 2: INR for Deblurring"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implicit neural representations can also be used for 2D image processing tasks like denoising, deblurring, inpainting, and super-resolution.\n",
        "\n",
        "In this section, we will explore INR for non-blind deblurring."
      ],
      "metadata": {
        "id": "9IpZQFr8kEBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 Define Blurring Kernel"
      ],
      "metadata": {
        "id": "7F2Xeh8yKNX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqxtfT0-6MLp"
      },
      "outputs": [],
      "source": [
        "# Creating and Applying Blurring Kernels\n",
        "# util: https://github.com/GaryMataev/DeepRED\n",
        "def fspecial_gauss(size, sigma):\n",
        "    \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function (Creates a Gaussian blurring kernel)\n",
        "    \"\"\"\n",
        "    x, y = np.mgrid[-size // 2 + 1:size // 2 + 1, -size // 2 + 1:size // 2 + 1]\n",
        "    g = np.exp(-((x ** 2 + y ** 2) / (2.0 * sigma ** 2)))\n",
        "    return g / g.sum()\n",
        "\n",
        "def get_fft_h(im, blur_type):\n",
        "    \"\"\"Computes the Fourier transform of a blurring kernel\n",
        "    \"\"\"\n",
        "    assert blur_type in ['uniform_blur', 'gauss_blur'], \"blur_type can be or 'uniform' or 'gauss'\"\n",
        "    ch, h, w = im.shape\n",
        "    fft_h    =  np.zeros((h,w),)\n",
        "    if blur_type=='uniform_blur':\n",
        "        t        =  4 # 9//2\n",
        "        fft_h[h//2-t:h//2+1+t, w//2-t:w//2+1+t]  = 1/81\n",
        "        fft_h    = np.fft.fft2(np.fft.fftshift(fft_h))\n",
        "    else: # gauss_blur\n",
        "        psf = fspecial_gauss(25, 1.6)\n",
        "        t = 12 # 25 // 2\n",
        "        fft_h[h//2-t:h//2+1+t, w//2-t:w//2+1+t]  = psf\n",
        "        fft_h    =  np.fft.fft2(np.fft.fftshift(fft_h))\n",
        "    return fft_h\n",
        "\n",
        "def blur(im, blur_type):\n",
        "    \"\"\"Apply blur kernel to the input image\n",
        "    \"\"\"\n",
        "    fft_h = get_fft_h(im, blur_type)\n",
        "    imout = np.zeros_like(im)\n",
        "    for i in range(im.shape[0]):\n",
        "        im_f    =  np.fft.fft2(im[i, :, :])\n",
        "        z_f     =  fft_h*im_f # .* of matlab\n",
        "        z       =  np.real(np.fft.ifft2(z_f))\n",
        "        imout[i, :, :] = z\n",
        "    return imout\n",
        "\n",
        "\n",
        "# - the inverse function H -\n",
        "def get_h(n_ch, blur_type, use_fourier, dtype):\n",
        "    assert blur_type in ['uniform_blur', 'gauss_blur'], \"blur_type can be or 'uniform' or 'gauss'\"\n",
        "    if not use_fourier:\n",
        "        return Downsampler(n_ch, 1, blur_type, preserve_size=True).type(dtype)\n",
        "    return lambda im: torch_blur(im, blur_type, dtype)\n",
        "\n",
        "\n",
        "def torch_blur(im, blur_type, dtype):\n",
        "  # blur an image tensor\n",
        "    fft_h = get_fft_h(torch_to_np(im), blur_type)\n",
        "    fft_h_torch = torch.unsqueeze(torch.from_numpy(np.real(fft_h)).type(dtype), 2)\n",
        "    fft_h_torch = torch.cat([fft_h_torch, fft_h_torch], 2)\n",
        "    z = []\n",
        "    for i in range(im.shape[1]):\n",
        "        im_torch = torch.unsqueeze(im[0, i, :, :], 2)\n",
        "        im_torch = torch.cat([im_torch, im_torch], 2)\n",
        "        im_f    =  torch.fft(im_torch, 2)\n",
        "        z_f     =  torch.mul(torch.unsqueeze(fft_h_torch, 0), torch.unsqueeze(im_f, 0)) # .* of matlab\n",
        "        z.append(torch.ifft(z_f, 2))\n",
        "    z = torch.cat(z, 0)\n",
        "    return torch.unsqueeze(z[:, :, :, 0], 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg0HC92Y6MLp"
      },
      "outputs": [],
      "source": [
        "def add_gaussian_noise(img, sigma):\n",
        "    # add noise on blur image\n",
        "    if sigma > 0:\n",
        "        noise = np.random.normal(scale=sigma , size=img.shape).astype(np.float32)\n",
        "\n",
        "        noisy_img = (img + noise).astype(np.float32)\n",
        "    else:\n",
        "        noisy_img = img.astype(np.float32)\n",
        "    #noisy_img=np.clip(noisy_img, 0.0, 1.0)\n",
        "    return noisy_img"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 Data Preparation"
      ],
      "metadata": {
        "id": "EqIq70yOKBys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhE6PsY96MLp"
      },
      "outputs": [],
      "source": [
        "def get_blur_image(img):\n",
        "\n",
        "    img = np.array(img).astype(np.float32)/255 # normalize to [0, 1]\n",
        "    img = img.transpose(2, 0, 1)\n",
        "\n",
        "    # blur\n",
        "    img = blur(img,blur_type=\"gauss_blur\")\n",
        "    sigma = 2**.5\n",
        "\n",
        "    # add noise\n",
        "    img = add_gaussian_noise(img, sigma/255)\n",
        "    img = np.clip(img,0,1) #clip to [0,1]\n",
        "\n",
        "    # scale to [-1, 1]\n",
        "    img = (img - 0.5)*2\n",
        "\n",
        "    plt.imshow(np.squeeze((img.transpose(1,2,0)+1)/2))\n",
        "    plt.pause(0.1)\n",
        "\n",
        "    blur_tensor = torch.tensor(img)\n",
        "    return blur_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKivTk556MLp"
      },
      "outputs": [],
      "source": [
        "# Blur Dataset\n",
        "class ImageBlur(Dataset):\n",
        "    def __init__(self,img):\n",
        "        super().__init__()\n",
        "\n",
        "        img = get_blur_image(img)\n",
        "\n",
        "        self.pixels = img.permute(1, 2, 0).view(-1, 3)\n",
        "\n",
        "        self.coords = get_mgrid(img.shape[1],img.shape[2],dim=2)\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx > 0: raise IndexError\n",
        "\n",
        "        return self.coords, self.pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w0cz4oH6MLp"
      },
      "outputs": [],
      "source": [
        "deblur_celeb = ImageBlur(image)\n",
        "deblur_dataloader = DataLoader(deblur_celeb, batch_size=1, pin_memory=True, num_workers=0)\n",
        "\n",
        "# create a siren model ready for deblurring\n",
        "deblur_siren = Siren(in_features=2, out_features=3, hidden_features=256,\n",
        "                  hidden_layers=6, outermost_linear=True)\n",
        "deblur_siren.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5fxh4ny6MLp"
      },
      "outputs": [],
      "source": [
        "class Downsampler(nn.Module):\n",
        "    \"\"\"\n",
        "        Resampling filters\n",
        "        http://www.realitypixels.com/turk/computergraphics/ResamplingFilters.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_planes, factor, kernel_type, phase=0, kernel_width=None, support=None, sigma=None,\n",
        "                 preserve_size=False, pad_type='reflection', transpose_conv=False):\n",
        "        super(Downsampler, self).__init__()\n",
        "\n",
        "        assert phase in [0, 0.5], 'phase should be 0 or 0.5'\n",
        "\n",
        "        if kernel_type == 'lanczos2':\n",
        "            support = 2\n",
        "            kernel_width = 4 * factor + 1\n",
        "            kernel_type_ = 'lanczos'\n",
        "\n",
        "        elif kernel_type == 'lanczos3':\n",
        "            support = 3\n",
        "            kernel_width = 6 * factor + 1\n",
        "            kernel_type_ = 'lanczos'\n",
        "\n",
        "        elif kernel_type == 'gauss12':\n",
        "            kernel_width = 7\n",
        "            sigma = 1 / 2\n",
        "            kernel_type_ = 'gauss'\n",
        "\n",
        "        elif kernel_type == 'gauss1sq2':\n",
        "            kernel_width = 9\n",
        "            sigma = 1. / np.sqrt(2)\n",
        "            kernel_type_ = 'gauss'\n",
        "\n",
        "        elif kernel_type == 'uniform_blur':\n",
        "            kernel_width = 9\n",
        "            kernel_type_ = 'uniform'\n",
        "            pad_type = 'circular'\n",
        "\n",
        "        elif kernel_type == 'gauss_blur':\n",
        "            kernel_width = 25\n",
        "            sigma = 1.6\n",
        "            kernel_type_ = 'gauss'\n",
        "            pad_type = 'circular'\n",
        "\n",
        "        elif kernel_type in {'lanczos', 'gauss', 'box'}:\n",
        "            kernel_type_ = kernel_type\n",
        "\n",
        "        else:\n",
        "            assert False, 'wrong name kernel'\n",
        "\n",
        "        # note that `kernel width` will be different to actual size for phase = 1/2\n",
        "        self.kernel = get_kernel(factor, kernel_type_, phase, kernel_width, support=support, sigma=sigma)\n",
        "        if transpose_conv:\n",
        "            if self.kernel.shape[0] % 2 == 1:\n",
        "                pad = int((self.kernel.shape[0] - 1) // 2.)\n",
        "            else:\n",
        "                pad = int((self.kernel.shape[0] - factor) // 2.)\n",
        "            downsampler = nn.ConvTranspose2d(n_planes, n_planes, kernel_size=self.kernel.shape,\n",
        "                                             stride=factor, padding=pad)\n",
        "        else:\n",
        "            downsampler = nn.Conv2d(n_planes, n_planes, kernel_size=self.kernel.shape, stride=factor, padding=0)\n",
        "        downsampler.weight.data[:] = 0\n",
        "        downsampler.bias.data[:] = 0\n",
        "\n",
        "        kernel_torch = torch.from_numpy(self.kernel)\n",
        "        for i in range(n_planes):\n",
        "            downsampler.weight.data[i, i] = kernel_torch\n",
        "\n",
        "        self.downsampler_ = downsampler\n",
        "\n",
        "        if preserve_size:\n",
        "            if pad_type=='circular':\n",
        "                self.padding = lambda torch_in: pad_circular(torch_in, kernel_width // 2)\n",
        "            elif pad_type=='reflection':\n",
        "                if self.kernel.shape[0] % 2 == 1:\n",
        "                    pad = int((self.kernel.shape[0] - 1) // 2.)\n",
        "                else:\n",
        "                    pad = int((self.kernel.shape[0] - factor) // 2.)\n",
        "                self.padding = nn.ReplicationPad2d(pad)\n",
        "            else:\n",
        "                assert False, \"pad_type have only circular or reflection options\"\n",
        "        self.preserve_size = preserve_size\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.preserve_size:\n",
        "            x = self.padding(input)\n",
        "        else:\n",
        "            x = input\n",
        "        self.x = x\n",
        "        return self.downsampler_(x)\n",
        "\n",
        "\n",
        "def get_kernel(factor, kernel_type, phase, kernel_width, support=None, sigma=None):\n",
        "    assert kernel_type in ['lanczos', 'gauss', 'box', 'uniform', 'blur']\n",
        "\n",
        "    # factor  = float(factor)\n",
        "    if phase == 0.5 and kernel_type != 'box':\n",
        "        kernel = np.zeros([kernel_width - 1, kernel_width - 1])\n",
        "    else:\n",
        "        kernel = np.zeros([kernel_width, kernel_width])\n",
        "\n",
        "    if kernel_type == 'box':\n",
        "        assert phase == 0.5, 'Box filter is always half-phased'\n",
        "        kernel[:] = 1. / (kernel_width * kernel_width)\n",
        "\n",
        "    elif kernel_type == 'gauss':\n",
        "        assert sigma, 'sigma is not specified'\n",
        "        assert phase != 0.5, 'phase 1/2 for gauss not implemented'\n",
        "        return fspecial_gauss(kernel_width, sigma)\n",
        "\n",
        "    elif kernel_type == 'uniform':\n",
        "        kernel = np.ones([kernel_width, kernel_width])\n",
        "\n",
        "    elif kernel_type == 'lanczos':\n",
        "        assert support, 'support is not specified'\n",
        "        center = (kernel_width + 1) / 2.\n",
        "\n",
        "        for i in range(1, kernel.shape[0] + 1):\n",
        "            for j in range(1, kernel.shape[1] + 1):#print metrics\n",
        "\n",
        "                if phase == 0.5:\n",
        "                    di = abs(i + 0.5 - center) / factor\n",
        "                    dj = abs(j + 0.5 - center) / factor\n",
        "                else:\n",
        "                    di = abs(i - center) / factor\n",
        "                    dj = abs(j - center) / factor\n",
        "\n",
        "                pi_sq = np.pi * np.pi\n",
        "\n",
        "                val = 1\n",
        "                if di != 0:\n",
        "                    val = val * support * np.sin(np.pi * di) * np.sin(np.pi * di / support)\n",
        "                    val = val / (np.pi * np.pi * di * di)\n",
        "\n",
        "                if dj != 0:\n",
        "                    val = val * support * np.sin(np.pi * dj) * np.sin(np.pi * dj / support)\n",
        "                    val = val / (np.pi * np.pi * dj * dj)\n",
        "                kernel[i - 1][j - 1] = val\n",
        "    else:\n",
        "        assert False, 'wrong method name'\n",
        "    kernel /= kernel.sum()\n",
        "    return kernel\n",
        "\n",
        "\n",
        "def pad_circular(x, pad):\n",
        "    \"\"\"\n",
        "    :param x: pytorch tensor of shape: [batch, ch, h, w]\n",
        "    :param pad: uint\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    x = torch.cat([x, x[:, :, 0:pad]], dim=2)\n",
        "    x = torch.cat([x, x[:, :, :, 0:pad]], dim=3)\n",
        "    x = torch.cat([x[:, :, -2*pad:-pad], x], dim=2)\n",
        "    x = torch.cat([x[:, :, :, -2*pad:-pad], x], dim=3)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH4kFbma6MLp"
      },
      "outputs": [],
      "source": [
        "# define blur_type\n",
        "blur_type = \"gauss_blur\"\n",
        "downsampler = Downsampler(3, 1, blur_type, preserve_size=True).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 Training and Deblurring\n",
        "\n",
        "**Task 4:** Visualize output"
      ],
      "metadata": {
        "id": "fa5GHD4kRKn2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aH_PP6pU6MLp"
      },
      "outputs": [],
      "source": [
        "total_steps = 2000\n",
        "steps_til_summary = 50\n",
        "\n",
        "optim = torch.optim.Adam(lr=1e-4, params=img_siren.parameters())\n",
        "\n",
        "model_input, ground_truth = next(iter(deblur_dataloader))\n",
        "model_input, ground_truth = model_input.cuda(), ground_truth.cuda()\n",
        "\n",
        "for step in range(total_steps):\n",
        "    model_output, coords = img_siren(model_input)\n",
        "\n",
        "    model_outputnew = (downsampler(model_output.view(1,218,178,3).permute(0,3,1,2))).permute(0,2,3,1).view(1,-1,3)\n",
        "\n",
        "    loss = ((model_outputnew - ground_truth)**2).mean()\n",
        "\n",
        "    if not step % steps_til_summary:\n",
        "        print(\"Step %d, Total loss %0.6f\" % (step, loss))\n",
        "\n",
        "        # Task 4 TODO\n",
        "\n",
        "\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpJCqqMc6MLp"
      },
      "source": [
        "## Section 3 Think about Memory Efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f9wK9LBMngC"
      },
      "source": [
        "**Challenge:**\n",
        "* Calculate and compare memory usage for traditional grid-based (pixel-based) representations and representations in INR.\n",
        "* What advantages might this offer in terms of efficiency for storing and representing high-dimensional data? Can you think of other potential applications of implicit neural representations (INR) beyond low-level vision tasks (e.g., denoising,debluring)?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}